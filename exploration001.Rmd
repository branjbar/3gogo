---
title: "Exploring a Price Index"
author: "Bijan Ranjbar-Sahraei"
date: "October 28, 2015 - January 5, 2016"
output:
  html_document:
    theme: united
    toc: yes
  pdf_document:
    toc: yes
---

Here is my mini-project on analysis of tHPI dataset. In the first try, late October 2015, The November and December prices were not determined yet, so I made predictions for them. Later, once the prices were anounced I compared my predictions with the final prices. Results were pretty okey, except the predictions of some cities such as Houston, Las Vegas and Denver for which prices had dropped further than expectations!  


# Data Processing
```{r echo=FALSE, message=FALSE}
rm(list=ls())  # to remove all variables from workspace 
library("reshape2")  # to reshape matrices to long lists
library("ggplot2")  # for plotting
library("NMF")  # for heatmaps
library("ggmap")  # for map plot 
library("gridExtra")  # for having multiple subplots in a grid
```

I start by importing a recent tHPI which shows the average price in USD for a standard double room, based on 25 of the most popular US cities. I also add the Latitude and Longitude of these 25 cities, which will be used for fancier visualizations. Here is the tHPI for top 6 cities of US.

```{r echo=FALSE}
data.csv <- read.csv("/Users/bian/sandbox/thpi_us_verify.csv")
data.tabular <- data.csv  # form now on I work with data.tabular and keep data.csv as the original version
head(data.tabular)

```

I reshape the spreadsheet data into a four column data frame, with city, month, price, and date in *first*, *second*, *third* and *forth* columns, respectively. The first six rows of the new data frame are as following.
```{r echo=FALSE}
data <- melt(data.tabular[,c(1:11)], id="city") # melts the matrix to a long list
data <- setNames(data[c("city", "variable", "value")], c("city", "month", "price"))  # changing the column names after melting them
data$date <- as.Date(paste("01",data$month,"15",sep=""),"%d%B%Y")   # converting the month names to date
head(data)
tail(data)


# for verification
data_verify <- melt(data.tabular[,c(1:13)], id="city") # melts the matrix to a long list
data_verify <- setNames(data_verify[c("city", "variable", "value")], c("city", "month", "price"))  # changing the column names after melting them
data_verify$date <- as.Date(paste("01",data_verify$month,"15",sep=""),"%d%B%Y")   # converting the month names to date


```
A few minor data processing tasks will be done later during the EDA and modeling phases. Next, I'll conduct *Exploratory Data Analysis (EDA)* for getting more insights into the dataset.

# Exploratory Data Analysis (EDA)
The following three plots, show the density of hotel prices.

```{r echo=FALSE}
ggplot(data, aes(x=price)) + geom_density()  # density of prices
ggplot(data, aes(x=price,color=month)) + geom_density()  # density of prices per month
ggplot(data, aes(x=price,color=city)) + geom_density()  # density of prices per city
```

According to the first plot, most of the hotel prices are around $150, however this price can go up to around $450. 
Based on the second plot, in January, most of the cities have the $150 price while in e.g., May the prices are much higher. 
The third plot, decomposes the price distribution of different cities. Which is hard to analyse as it is. Therefore, I continue with the following two boxplots.

```{r echo=FALSE}
# a boxplot of prices per month
ggplot(data, aes(x=month,y=price)) + geom_boxplot() + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

# a boxplot of prices per city (too many cities so it runs out of borders)
ggplot(data, aes(x=city,y=price)) + geom_boxplot() + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))

#ggplot(data, aes(x=date,y=price)) + geom_point() + geom_smooth()
#ggplot(data, aes(x=date,y=price,color=city)) + geom_point() + geom_smooth(se=FALSE)
```

Actually, the above two plots became my favorites. The former one, illustrates the high-season. The average price starts increasing in April and is maximum in July. Then drops in August. 

The latter plot, immediately shows different type of cities. *Boston* and *New York* have very high prices, and their prices vary very much. While, *Houston* or *Orlando* are cheaper and much more stable. 

The following heatmap illustrates how prices change across months and cities. 

```{r echo=FALSE}
nba <- data.tabular  # to make the heatmap I need a matrix of prices per month and city
nba <- nba[,2:11]

nba <- sapply(nba,function(x) {as.numeric(gsub("[^[:digit:]]",'',x))})  # to extract prices if any $-sign or something like that exists in it
row.names(nba) <- data.tabular$city  # bringing back the row numbers to nba

nba_matrix <- data.matrix(nba)
aheatmap(nba_matrix, Rowv=FALSE, Colv=FALSE, fontsize=5, cexRow=2, cexCol=2)

# normalized based on cities
#aheatmap(nba_matrix, color = "-RdBu:50", scale = "column", Rowv=FALSE, Colv=FALSE,fontsize=5, cexRow=2, cexCol=2)

# normalized based on months
# aheatmap(nba_matrix, color = "-RdBu:50", scale = "row", Rowv=FALSE, Colv=FALSE,fontsize=3, cexRow=3, cexCol=2)
```

The lower right corner of the above heatmap, roughly, captures the cities and months which I see the highest prices. In this heatmap, I can also differentiate between cities with stable prices and cities with very varying prices. 


```{r echo=FALSE, message=FALSE, warning=FALSE}
# creating a sample data.frame with lat/lon points
lon <- data.tabular$lon
lat <- data.tabular$lat
df <- as.data.frame(cbind(lon,lat))

# getting the map from google!
mapgilbert <- get_map(location = c(lon = mean(df$lon), lat = mean(df$lat)), zoom = 4, maptype = "terrain", scale = 1)

# plotting the map with some points on it
data.tabular$sept.price <- as.numeric(gsub("[^[:digit:]]",'',data.tabular$September))
data.tabular$oct.price <- as.numeric(gsub("[^[:digit:]]",'',data.tabular$October))
data.tabular$aug.price <- as.numeric(gsub("[^[:digit:]]",'',data.tabular$August))

# ggmap(mapgilbert) + geom_point(data = data.tabular, aes(x = lon, y = lat, fill = oct.price,label=city), size = 5, shape = 21)

#ggmap(mapgilbert) +
#  geom_point(data = data.tabular, aes(x = lon, y = lat, fill = sept.price),size = 5, shape = 21) + 
#  geom_point(data = data.tabular, aes(x = lon, y = lat+.7, fill = oct.price),size = 5, shape = 22) + 
#  geom_point(data = data.tabular, aes(x = lon+.7, y = lat, fill = aug.price),size = 5, shape = 24) + 
#  scale_fill_gradient2(low = "#0000FF", high ="#FF0000", midpoint = 250)


#ggplot(data = data.tabular, aes(x = lon, y = lat,label=city)) + 
#  geom_point(aes(fill = oct.price),size = 5, shape = 21) + 
#  geom_text(aes(label=city),hjust=0, vjust=1.5) + 
#  scale_fill_gradient2(low = "#0000FF", high ="#FF0000", midpoint = 250)

```

Here I try to make categories for cities. First, I categorize each city as "Cheap Cities", "Moderate Cities" and "Expensive Cities". Second, I categorize them as "Stable Cities" and "Unstable Cities". Before this categorization, let me extract the mean, standard deviation and range of prices from January to October, for each city. Following comes these statistics for the top 6 cities of US. 

```{r echo=FALSE}
# getting price statistics

data.summary = data.frame(city=unique(data$city))  # a data frame for summary of data

d <- aggregate(data$price, by=list(city=data$city), FUN=mean)  # temporary mean 
data.summary$price.mean <- sapply(data.summary$city,FUN=function(x){d[d$city==x,]$x})

d <- aggregate(data$price, by=list(city=data$city), FUN=sd)  # temporary sd
data.summary$price.sd <- sapply(data.summary$city,FUN=function(x){d[d$city==x,]$x})

d <- aggregate(data$price, by=list(city=data$city), FUN=range)  # temporary range
data.summary$price.range <- sapply(data.summary$city,FUN=function(x){d[d$city==x,]$x[2] - d[d$city==x,]$x[1]})

# assinging price classes 
data.summary$price.mean.class <- cut(data.summary$price.mean,breaks=3)
data.summary$price.sd.class <- cut(data.summary$price.sd,breaks=2)
data.summary$price.range.class <- cut(data.summary$price.range,breaks=2)
head(data.summary)

# let's check the price distributions
#ggplot(data=data.summary, aes(x=price.mean.class,y=price.mean)) + geom_boxplot()
#ggplot(data=data.summary, aes(x=price.sd.class,y=price.sd)) + geom_boxplot()
#ggplot(data=data.summary, aes(x=price.range.class,y=price.range)) + geom_boxplot()

```

```{r echo=FALSE}
# plotting three point for each city based on prices in different months
data.summary$lat <- sapply(data.summary$city,function(x) {as.numeric(data.tabular[data.tabular$city == x,]$lat[1])})
data.summary$lon <- sapply(data.summary$city,function(x) {as.numeric(data.tabular[data.tabular$city == x,]$lon[1])})
#ggplot(data=data.summary, aes(x=lon,y=lat,color=price.mean.class)) + geom_point(aes(shape=price.sd.class),size=4) + geom_text(aes(label=city),hjust=0, vjust=0) 
```

The change of prices in each category are shown in following plots. 

Fries: Cheap, Pizza: Moderate, Steak: Expensive
Horse: Stable, Bull: Unstable

```{r echo=FALSE, warning=FALSE, message=FALSE}

data.summary$class.price[data.summary$price.mean <= 210] <- "cheap" 
data.summary$class.price[data.summary$price.mean <= 284 & data.summary$price.mean > 210] <- "moderate" 
data.summary$class.price[data.summary$price.mean > 284] <- "expensive" 

#data.summary$class.stability[data.summary$price.sd <= 44.5] <- "stable" 
#data.summary$class.stability[data.summary$price.sd > 44.5] <- "unstable" 

data.summary$class.stability[data.summary$price.range <= 100] <- "stable" 
data.summary$class.stability[data.summary$price.range > 100] <- "unstable" 

# plotting cheap, moderate and expensive cities

ggplot(subset(data,city %in% data.summary$city[data.summary$class.price == "cheap"]), aes(x=date,y=price,color=city)) +
  geom_point() + geom_smooth(se=FALSE) + 
  ggtitle("Fries Cities") + 
  scale_y_continuous(limits = c(90,500))

ggplot(subset(data,city %in% data.summary$city[data.summary$class.price == "moderate"]), aes(x=date,y=price,color=city)) + 
  geom_point() + geom_smooth(se=FALSE) + 
  ggtitle("Pizza Cities") + 
  scale_y_continuous(limits = c(90,500))

ggplot(subset(data,city %in% data.summary$city[data.summary$class.price == "expensive"]), aes(x=date,y=price,color=city)) + 
  geom_point() + geom_smooth(se=FALSE) + 
  ggtitle("Steak Cities") + 
  scale_y_continuous(limits = c(90,500))

# plotting stable and unstable cities

ggplot(subset(data,city %in% data.summary$city[data.summary$class.stability == "stable"]), aes(x=date,y=price,color=city)) + 
  geom_point() + geom_smooth(se=FALSE) + 
  ggtitle("Horse Cities") + 
  scale_y_continuous(limits = c(90,500))

ggplot(subset(data,city %in% data.summary$city[data.summary$class.stability == "unstable"]), aes(x=date,y=price,color=city)) + 
  geom_point() + geom_smooth(se=FALSE) + 
  ggtitle("Bull Cities") + 
  scale_y_continuous(limits = c(90,500))


```

Using all the above plots, my intuition is that fluctuation of prices are following certain patterns. I illustrate some patterns, namely *up-down-up, up-up-down-up, up-up-up-up, up-up-up-down* in the following four subplots. 

```{r echo=FALSE, warning=FALSE, message=FALSE}
# making categories for prices.
data$trend[data$city %in% c("Phoenix","Miami")] <- "updownup"
data$trend[data$city %in% c("Boston","Meanneapolis","Chicago","New York")] <- "upupdownup"
data$trend[data$city %in% c("San Francisco")] <- "upupup"
data$trend[data$city %in% c("Seattle")] <- "upupupdown"

# plotting each category
p1 <- ggplot(subset(data, trend=="updownup"), aes(x=date,y=price,color=city)) + 
  geom_point() + 
  geom_smooth(se=FALSE,aes(color=city)) + 
  ggtitle("up-down-up") + 
  scale_y_continuous(limits = c(90,500))
p2 <- ggplot(subset(data, trend=="upupdownup"), aes(x=date,y=price,color=city)) + 
  geom_point() + 
  geom_smooth(se=FALSE,aes(color=city)) + 
  ggtitle("up-up-down-up") + 
  scale_y_continuous(limits = c(90,500))
p3 <- ggplot(subset(data, trend=="upupup"), aes(x=date,y=price,color=city)) + 
  geom_point() + 
  geom_smooth(se=FALSE,aes(color=city)) + 
  ggtitle("up-up-up-up") + 
  scale_y_continuous(limits = c(90,500))
p4 <- ggplot(subset(data, trend=="upupupdown"), aes(x=date,y=price,color=city)) + 
  geom_point() + 
  geom_smooth(se=FALSE,aes(color=city)) + 
  ggtitle("up-up-up-down") + 
  scale_y_continuous(limits = c(90,500))

grid.arrange(p1,p2,p3,p4)

```

Based on my observations in above plots, and considering that the traveling patterns follow the climate changes, school holidays (and other yearly and 6-monthly periodic events), I expect that a linear combination of some *Sinus* and *Cosinus* functions with periods of 12 months and 6 months can estimate the trend of prices.


# Modeling
It seems that every city is following a periodic trend. For this, I define these four predictors: $sin(\frac{\pi t}{6}),cos(\frac{\pi t}{6}),sin(\frac{\pi t}{3}),cos(\frac{\pi t}{3})$ where $t$ is the number of month ranging from 1 to 12 for January to December. Then, I use a linear model to predict the price of each month. See following two R functions.

```{r message = F}
# generating predictors for linear models
get.predictors <- function(t) {
  return(data.frame(t=t,
           sin = sin(pi/6*t), # 12 month period
           cos = cos(pi/6*t),
           sin2 = sin(pi/3*t), # 6 month period
           cos2 = cos(pi/3*t)
           )
         )
}

# predicting prices based on the existing prices
get.predictions <- function(price.trend,time.end) {
  lmfit <- lm(y ~ sin+cos+sin2+cos2,data=price.trend)
  prediction.time <- 1:time.end
  return(price.prediction <- data.frame(t=prediction.time,y=predict(lmfit,get.predictors(prediction.time))))
}
```

```{r echo=FALSE}
# a function to get coefficients of the model
get.coeffs <- function(price.trend,time.end) {
  lmfit <- lm(y ~ sin+cos+sin2+cos2,data=price.trend)
  return(lmfit$coefficients)
}

```

## Estimating October Prices (Testing)
To test this model I train one model for each city, based on the prices from January to September, and then compare its predictions for October prices with the real values. 

After visually testing the predicted prices of October, I realized that for some of the cities the predictions are very promissing, while for some other cities predictions fail.

```{r echo=FALSE}

# for every city plot the prediction in put them in p
p <- lapply(data.summary$city,function(the_city) {
  price.train = get.predictors(1:9)  # getting the predictors
  price.train$y = data$price[data$city==the_city][1:9]  # getting the ouput for training data
  price.trend = get.predictors(1:10) 
  price.trend$y = data$price[data$city==the_city][1:10]  
  
  
  price.prediction <- get.predictions(price.train,10)
  
  ggplot(price.trend, aes(x=t,y=y)) + 
      geom_point() + 
      geom_point(data=price.prediction, aes(x=t,y=y),color="red") + 
      geom_line(data=price.prediction, aes(x=t,y=y),color="red") + 
      geom_line() +
      ggtitle(the_city)
      
})
```

Some good predictions of October price are shown in following plots (the black dots are real values and the red dots and lines are my predictions).

```{r echo=FALSE}
do.call(grid.arrange, c(p[c(3,19,22)], list(ncol=3)))  # I need this do.call to take out the plots from p
```

For some other cities, the predictions don't work that well. This can be because of unknown predictors that our model doesn't take into account, or other reasons! 

```{r echo=FALSE}
do.call(grid.arrange, c(p[c(1,12,20,23)], list(ncol=2)))
```

Overall, the predictions are not very bad; the trend of prices is usually followed by the model. Therefore, next, I use my model to predict the prices of November and December. 

## Verification
I use all available prices from January to October to train my model and then I make predictions for November and December prices. The predictions for all 25 US cities are illustrated in following (The black dots and lines show the real prices and red lines show the predictions).

```{r echo=FALSE}
# plotting the predictions
p <- lapply(data.summary$city,function(the_city) {
  
  # training
  price.train = get.predictors(1:10)
  price.train$y = data$price[data$city==the_city][1:10]
  
  # to verify
  price.verify = get.predictors(1:12)
  price.verify$y = data_verify$price[data_verify$city==the_city][1:12]
  
  # predictions
  price.trend = get.predictors(1:10)
  price.trend$y = data$price[data$city==the_city][1:10]
  price.prediction <- get.predictions(price.train,12)
  
  ggplot(price.verify, aes(x=t,y=y)) + 
      geom_point() + 
      geom_line(data=price.prediction, aes(x=t,y=y),color="red",size=1.4) + 
      geom_line() +
      ggtitle(the_city)
      
})
do.call(grid.arrange, c(p[c(1:9)], list(ncol=3)))
do.call(grid.arrange, c(p[c(10:18)], list(ncol=3)))
do.call(grid.arrange, c(p[c(19:25)], list(ncol=3)))
```

Following comes my predictions for prices in next months which can be compared with the real prices shown next to them.

```{r echo=FALSE}
# predicting november and december prices!
data.estimated <- data.csv[c(1:13)]
for (the_city in data.summary$city) {
  price.train = get.predictors(1:10)
  price.train$y = data$price[data$city==the_city][1:10]
  price.prediction <- get.predictions(price.train,12)
  data.estimated$November_aprox[data.tabular$city==the_city] <- round(price.prediction$y[11])
  data.estimated$December_aprox[data.tabular$city==the_city] <- round(price.prediction$y[12])
}

data.estimated[c("city","November", "November_aprox", "December", "December_aprox")]

```


## Clustering based on the Periodic Model
While training the models, for each city I get 4 model coefficients. I assume that these coefficients are the features for the cities. This allows me to use a K-means clustering algorithm to see if the coefficients can help me to make clusters for the cities.  


```{r echo=FALSE}
# clustering based on data coefficients
data.coeffs <- data.frame(city=data.summary$city)
for (the_city in data.summary$city) {
  price.train = get.predictors(1:10)
  price.train$y = data$price[data$city==the_city][1:10]
  coeffs <- get.coeffs(price.train,10)
  data.coeffs$sin[data.coeffs$city == the_city] <- coeffs["sin"]
  data.coeffs$cos[data.coeffs$city == the_city] <- coeffs["cos"]
  data.coeffs$sin2[data.coeffs$city == the_city] <- coeffs["sin2"]
  data.coeffs$cos2[data.coeffs$city == the_city] <- coeffs["sin2"]
}

data.clusters <- kmeans(data.coeffs[2:5],centers=4,iter.max=10)
data.tabular$cluster <- factor(data.clusters$cluster)
```

A simple model with *k=4* clusters have a goodness of `r round(data.clusters$betweenss / data.clusters$totss * 1000) / 10`%. The following figure illustrates the 25 cities on a map and each cluster is shown by a color.

```{r echo=FALSE}
# plotting the cities using lat-lon based on clusters
ggmap(mapgilbert) + geom_point(data=data.tabular, aes(x=lon, y=lat,fill=cluster), alpha=.8,size = 8, shape = 21) 

```


The above map already captures some of the geographical and climate characteristics of cities. For instance, Seattle which indeed has a different climate from other cities is the single member of its own cluster. Besides, many cities in central and western parts of the US which have similar climates end up in the same cluster. These clusters should be studied further in the future. 


# Summary and Conclusion
In this technical report, I started by importing the tHPI data and applied some basic modifications and reshaping to data. Then I conducted EDA to get insights into this dataset. As a result of my EDA I realized that prices follow periodic patterns. Therefore, I decided to use $Sinus$ and $Cosinus$ functions to fit a model to data. First, I tested my model by leaving out the October prices from training data and then I *visually* compared my predictions with the real prices. Then, I trained my model based on all available prices and made predictions for November and December prices. Also, coefficients of my model helped me to summarize each city with four numeric features which I used to  cluster the cities. 

I emphasize that the analysis provided in this report is very basic, and the choices might be naive in some cases. To improve the results, more domain knowledge, more data and deeper analysis are required. Next section elaborates on some of the possible future improvements.

# Future Perspective
If I find more time, I'll do the following.

1. Using other tHPI Datasets
  - The *US tHPI data archive* is available for 2012, 2013 and 2014. I can use it for getting further insights and to train a better model.
  - I can use similar datasets for Europe and Canada  as well.
2. Collecting new Data
  - According to this report, hotel prices vary based on seasons. Therefore, collecting the temperature and raining trends can help in predicting the prices. Different starting time of seasons in different areas of US can be the reason of different price patterns.
  - For each city, data on population of cities, number of tourists and number of hotel rooms exist! Such data can explain why in some cities prices are very *unstable* and for other cities they don't change much!
3. Feedback loop - We should be careful with the effect of our predictions on future prices. The prices are determined in a negotiating process; by making predictions of future prices we might influence this negotiation process and this can result in emergence of new patterns.
4. Pricing Anomalies -  If I look at the monthly estimations and real world trends, I see some anomalies! For example in *Houston* the prices in May and June show a significant deviation from the expected periodic pattern. Also, in *Atlanta* prices of July and August deviate. July of *San Diago* and *San Antonio* should be explored more, too. 
5. The Periodic model I trained in this report was tested by leaving out the prices of October and then comparing estimations with real values for this month. However, I didn't provide a concrete accuracy measure for this model. This can be further explored. 

  
  
